// Este archivo manejarÃ¡ la lÃ³gica para interactuar con la API de OpenAI, como la creaciÃ³n de embeddings y generaciÃ³n de respuestas.

import OpenAI from "openai";
import * as dotenv from "dotenv";

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// âœ… Generar embeddings
async function generateEmbeddings(text: string): Promise<number[]> {
  try {
    console.log("ğŸ“Œ Generando embeddings para:", text);
    const response = await openai.embeddings.create({
      model: process.env.OPENAI_EMBEDDING_MODEL!,
      input: text,
    });

    return response.data[0].embedding;
  } catch (error) {
    console.error("âŒ Error generando embeddings:", error);
    throw new Error("Error generando embeddings");
  }
}

// âœ… Generar respuesta con GPT usando fragmentos optimizados
async function generateResponse(userQuery: string, context: string = ""): Promise<string> {
  try {
    console.log("ğŸ“Œ Enviando a OpenAI solo los fragmentos relevantes...");

    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: context
            ? `Responde la pregunta del usuario solo en base a la siguiente informaciÃ³n: ${context}`
            : "Responde la pregunta del usuario de manera clara y precisa.",
        },
        {
          role: "user",
          content: userQuery,
        },
      ],
      max_tokens: 100, // ğŸ”¥ Reducimos el lÃ­mite de tokens para optimizar costos
      temperature: 0.5,
    });

    return response.choices[0]?.message?.content || "No tengo informaciÃ³n suficiente.";
  } catch (error) {
    console.error("âŒ Error generando respuesta con OpenAI:", error);
    return "Hubo un error generando la respuesta.";
  }
}

export { generateEmbeddings, generateResponse };
